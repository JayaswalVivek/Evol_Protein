{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Mode_final</th>\n",
       "      <th>Length</th>\n",
       "      <th>Ntranscripts</th>\n",
       "      <th>ENC</th>\n",
       "      <th>TauIndex</th>\n",
       "      <th>TissueSpecific</th>\n",
       "      <th>Expression</th>\n",
       "      <th>Chr2</th>\n",
       "      <th>Recombination</th>\n",
       "      <th>Secretion</th>\n",
       "      <th>ImmResp</th>\n",
       "      <th>Protease</th>\n",
       "      <th>SpermComp2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FBgn0262006</td>\n",
       "      <td>Relaxed</td>\n",
       "      <td>464</td>\n",
       "      <td>1</td>\n",
       "      <td>68.527</td>\n",
       "      <td>0.998904</td>\n",
       "      <td>1</td>\n",
       "      <td>AG</td>\n",
       "      <td>A</td>\n",
       "      <td>1.741026</td>\n",
       "      <td>nottransfered</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FBgn0036154</td>\n",
       "      <td>Relaxed</td>\n",
       "      <td>2078</td>\n",
       "      <td>2</td>\n",
       "      <td>61.876</td>\n",
       "      <td>0.998979</td>\n",
       "      <td>1</td>\n",
       "      <td>AG</td>\n",
       "      <td>A</td>\n",
       "      <td>3.045938</td>\n",
       "      <td>nottransfered</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FBgn0036158</td>\n",
       "      <td>Relaxed</td>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>55.426</td>\n",
       "      <td>0.999364</td>\n",
       "      <td>1</td>\n",
       "      <td>AG</td>\n",
       "      <td>A</td>\n",
       "      <td>3.184389</td>\n",
       "      <td>nottransfered</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBgn0031468</td>\n",
       "      <td>Relaxed</td>\n",
       "      <td>2515</td>\n",
       "      <td>1</td>\n",
       "      <td>51.827</td>\n",
       "      <td>0.980826</td>\n",
       "      <td>1</td>\n",
       "      <td>AG</td>\n",
       "      <td>A</td>\n",
       "      <td>2.228226</td>\n",
       "      <td>transfered</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FBgn0015583</td>\n",
       "      <td>Relaxed</td>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>57.888</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>1</td>\n",
       "      <td>AG</td>\n",
       "      <td>A</td>\n",
       "      <td>3.283702</td>\n",
       "      <td>transfered</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>FBgn0259975</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>396</td>\n",
       "      <td>1</td>\n",
       "      <td>56.411</td>\n",
       "      <td>0.991108</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>A</td>\n",
       "      <td>1.305770</td>\n",
       "      <td>transfered</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>FBgn0259968</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>451</td>\n",
       "      <td>2</td>\n",
       "      <td>39.309</td>\n",
       "      <td>0.990510</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>transfered</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>FBgn0038014</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "      <td>60.105</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>1</td>\n",
       "      <td>AG</td>\n",
       "      <td>A</td>\n",
       "      <td>1.741026</td>\n",
       "      <td>transfered</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>FBgn0083966</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>974</td>\n",
       "      <td>1</td>\n",
       "      <td>59.916</td>\n",
       "      <td>0.999227</td>\n",
       "      <td>1</td>\n",
       "      <td>AG</td>\n",
       "      <td>A</td>\n",
       "      <td>1.196956</td>\n",
       "      <td>transfered</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>FBgn0034154</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>751</td>\n",
       "      <td>1</td>\n",
       "      <td>59.649</td>\n",
       "      <td>0.990924</td>\n",
       "      <td>1</td>\n",
       "      <td>AG</td>\n",
       "      <td>A</td>\n",
       "      <td>3.579318</td>\n",
       "      <td>transfered</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name   Mode_final  Length  Ntranscripts     ENC  TauIndex  \\\n",
       "0    FBgn0262006      Relaxed     464             1  68.527  0.998904   \n",
       "1    FBgn0036154      Relaxed    2078             2  61.876  0.998979   \n",
       "2    FBgn0036158      Relaxed     782             1  55.426  0.999364   \n",
       "3    FBgn0031468      Relaxed    2515             1  51.827  0.980826   \n",
       "4    FBgn0015583      Relaxed     767             1  57.888  0.999111   \n",
       "..           ...          ...     ...           ...     ...       ...   \n",
       "249  FBgn0259975  Constrained     396             1  56.411  0.991108   \n",
       "250  FBgn0259968  Constrained     451             2  39.309  0.990510   \n",
       "251  FBgn0038014  Constrained    1289             1  60.105  0.999002   \n",
       "252  FBgn0083966  Constrained     974             1  59.916  0.999227   \n",
       "253  FBgn0034154  Constrained     751             1  59.649  0.990924   \n",
       "\n",
       "     TissueSpecific Expression Chr2  Recombination      Secretion ImmResp  \\\n",
       "0                 1         AG    A       1.741026  nottransfered       N   \n",
       "1                 1         AG    A       3.045938  nottransfered       Y   \n",
       "2                 1         AG    A       3.184389  nottransfered       N   \n",
       "3                 1         AG    A       2.228226     transfered       N   \n",
       "4                 1         AG    A       3.283702     transfered       N   \n",
       "..              ...        ...  ...            ...            ...     ...   \n",
       "249               1      other    A       1.305770     transfered       N   \n",
       "250               1      other    A       0.000000     transfered       N   \n",
       "251               1         AG    A       1.741026     transfered       N   \n",
       "252               1         AG    A       1.196956     transfered       N   \n",
       "253               1         AG    A       3.579318     transfered       N   \n",
       "\n",
       "    Protease SpermComp2  \n",
       "0          N          N  \n",
       "1          N          N  \n",
       "2          N          N  \n",
       "3          N          N  \n",
       "4          N          Y  \n",
       "..       ...        ...  \n",
       "249        N          Y  \n",
       "250        N          Y  \n",
       "251        Y          N  \n",
       "252        Y          Y  \n",
       "253        N          Y  \n",
       "\n",
       "[254 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the input data set\n",
    "io_folder = 'C:/UCI/Predictive_Modelling/Data/{}'\n",
    "\n",
    "input_data = pd.read_csv(io_folder.format('Zambia.csv')) # Replace Zambia.csv with Raleigh.csv, if required\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the variables to be used for model development\n",
    "continuous_var = ['Length', 'Ntranscripts', 'TauIndex', 'Recombination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Ntranscripts</th>\n",
       "      <th>TauIndex</th>\n",
       "      <th>Recombination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>464</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998904</td>\n",
       "      <td>1.741026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2078</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998979</td>\n",
       "      <td>3.045938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999364</td>\n",
       "      <td>3.184389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.980826</td>\n",
       "      <td>2.228226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999111</td>\n",
       "      <td>3.283702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>396</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991108</td>\n",
       "      <td>1.305770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>451</td>\n",
       "      <td>2</td>\n",
       "      <td>0.990510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>1.741026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>974</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999227</td>\n",
       "      <td>1.196956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>751</td>\n",
       "      <td>1</td>\n",
       "      <td>0.990924</td>\n",
       "      <td>3.579318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Length  Ntranscripts  TauIndex  Recombination\n",
       "0       464             1  0.998904       1.741026\n",
       "1      2078             2  0.998979       3.045938\n",
       "2       782             1  0.999364       3.184389\n",
       "3      2515             1  0.980826       2.228226\n",
       "4       767             1  0.999111       3.283702\n",
       "..      ...           ...       ...            ...\n",
       "249     396             1  0.991108       1.305770\n",
       "250     451             2  0.990510       0.000000\n",
       "251    1289             1  0.999002       1.741026\n",
       "252     974             1  0.999227       1.196956\n",
       "253     751             1  0.990924       3.579318\n",
       "\n",
       "[254 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the sub-matrix for model building and testing\n",
    "sub_matrix = input_data[continuous_var].copy()\n",
    "sub_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the number of input variables\n",
    "num_var = sub_matrix.shape[1]\n",
    "num_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Constrained', 'Positive', 'Relaxed'], dtype=object),\n",
       " array([ 99,  31, 124], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of observations per class\n",
    "np.unique(input_data['Mode_final'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map class labels to indexes\n",
    "response_variable = input_data['Mode_final'].replace(to_replace = ['Constrained', 'Positive', 'Relaxed']\n",
    "                                                     , value = [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the weights per class as a Tensor\n",
    "# weight_per_class = torch.tensor([1., 3., 0.8])\n",
    "weight_per_class = torch.tensor([1., 1., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the splitting of data into batches\n",
    "# num_batch = np.ceil(X_train_scaled.shape[0]/32).astype(int)\n",
    "num_batch = 1\n",
    "\n",
    "# grp_indexes = np.random.randint(num_batch, size=X_train_scaled.shape[0])\n",
    "# np.unique(grp_indexes, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        # call constructor from superclass\n",
    "        super().__init__()\n",
    "        \n",
    "        # define network layers\n",
    "        self.fc1 = torch.nn.Linear(num_inputs, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_model = Logistic_Regression(num_var)\n",
    "nnet_loss_func = nn.CrossEntropyLoss(weight = weight_per_class) #All classes are treated approximately equally\n",
    "nnet_optimizer = torch.optim.Adam(nnet_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of model parameters\n",
    "num_param = 0\n",
    "\n",
    "for param_itr in nnet_model.parameters():\n",
    "    num_param += param_itr.numel()\n",
    "    \n",
    "num_param    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Neural Network with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hidden_Layer_Two(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        # call constructor from superclass\n",
    "        super().__init__()\n",
    "        \n",
    "        # define network layers\n",
    "        self.fc1 = torch.nn.Linear(num_inputs, 3)\n",
    "        self.fc2 = torch.nn.Linear(3, 3)\n",
    "        self.fc3 = torch.nn.Linear(3, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet_model = Hidden_Layer_Two(num_var)\n",
    "hnet_loss_func = nn.CrossEntropyLoss(weight = weight_per_class) #All classes are treated approximately equally\n",
    "hnet_optimizer = torch.optim.Adam(hnet_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of model parameters\n",
    "num_param = 0\n",
    "\n",
    "for param_itr in hnet_model.parameters():\n",
    "    num_param += param_itr.numel()\n",
    "    \n",
    "num_param  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Results\n",
    "Based on 100 iterations using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration =  0\n",
      "Iteration =  10\n",
      "Iteration =  20\n",
      "Iteration =  30\n",
      "Iteration =  40\n",
      "Iteration =  50\n",
      "Iteration =  60\n",
      "Iteration =  70\n",
      "Iteration =  80\n",
      "Iteration =  90\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(100)\n",
    "\n",
    "per_epoch_loss = []\n",
    "test_accuracy_list = []\n",
    "type_0_accuracy = []\n",
    "type_1_accuracy = []\n",
    "type_2_accuracy = []\n",
    "\n",
    "for itr_count in range(100):\n",
    "\n",
    "    if itr_count%10 ==0:\n",
    "        print('Iteration = ', itr_count)\n",
    "    \n",
    "    # Generate test-train split\n",
    "    split_seed = rng.integers(2**30)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sub_matrix, response_variable, test_size=0.33\n",
    "                                                        , random_state=split_seed, stratify=response_variable)\n",
    "\n",
    "    # Use ONLY training set to estimate the parameters for data normalization\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[continuous_var])\n",
    "\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    X_train_scaled[continuous_var] = scaler.transform(X_train[continuous_var])\n",
    "    X_test_scaled[continuous_var] = scaler.transform(X_test[continuous_var])\n",
    "    grp_indexes = np.random.randint(num_batch, size=X_train_scaled.shape[0])\n",
    "\n",
    "    # Convert training and test data sets to tensors\n",
    "    X_train_scaled = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values)\n",
    "\n",
    "    X_test_scaled = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "\n",
    "    # Perform logistic regression\n",
    "    nnet_model = Logistic_Regression(num_var)\n",
    "    nnet_loss_func = nn.CrossEntropyLoss(weight = weight_per_class) # Up- or down-sample observations to reduce class imbalance\n",
    "    nnet_optimizer = torch.optim.Adam(nnet_model.parameters(), lr=0.1)\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        for batch_idx in range(num_batch):\n",
    "            \n",
    "            current_batch_idx = np.where(grp_indexes == batch_idx)[0].tolist()\n",
    "            current_fit = nnet_model(X_train_scaled[current_batch_idx, :])  # 1\n",
    "            current_loss = nnet_loss_func(current_fit, y_train[current_batch_idx])  # 2\n",
    "            nnet_optimizer.zero_grad()  # 3\n",
    "            current_loss.backward()  # 4\n",
    "            nnet_optimizer.step()  # 5\n",
    "            per_epoch_loss.append(current_loss.detach().numpy().tolist())\n",
    "    \n",
    "    test_predictions = torch.argmax(torch.softmax(nnet_model(X_test_scaled), 1), axis=1).numpy()\n",
    "    test_accuracy = float(sum(test_predictions == y_test))/y_test.shape[0]\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "        \n",
    "    # Obtain the accuracy per class\n",
    "    zero_idx = np.where(y_test == 0)[0] \n",
    "    one_idx = np.where(y_test == 1)[0] \n",
    "    two_idx = np.where(y_test == 2)[0]\n",
    "    \n",
    "    type_0_accuracy.append(sum(test_predictions[zero_idx] == 0)/len(zero_idx))\n",
    "    type_1_accuracy.append(sum(test_predictions[one_idx] == 1)/len(one_idx))\n",
    "    type_2_accuracy.append(sum(test_predictions[two_idx] == 2)/len(two_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results for plotting using ggplot\n",
    "# accuracy_frame = pd.DataFrame(test_accuracy_list)\n",
    "# accuracy_frame.to_csv('Overall_Accuracy_Zambia.csv', index=False, header=False)\n",
    "\n",
    "# accuracy_frame = pd.DataFrame(type_0_accuracy)\n",
    "# accuracy_frame.to_csv('Type_0_Accuracy_Zambia.csv', index=False, header=False)\n",
    "\n",
    "# accuracy_frame = pd.DataFrame(type_1_accuracy)\n",
    "# accuracy_frame.to_csv('Type_1_Accuracy_Zambia.csv', index=False, header=False)\n",
    "\n",
    "# accuracy_frame = pd.DataFrame(type_2_accuracy)\n",
    "# accuracy_frame.to_csv('Type_2_Accuracy_Zambia.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47619048, 0.57142857, 0.60119048, 0.63095238, 0.66666667])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the 5-figure summary\n",
    "np.percentile(test_accuracy_list, [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.63095238095239"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_accuracy_list) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3030303  0.42424242 0.48484848 0.51515152 0.75757576]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.75757575757575"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acuracy per class - Class 0\n",
    "print(np.percentile(type_0_accuracy, [0, 25, 50, 75, 100]))\n",
    "\n",
    "np.mean(type_0_accuracy) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.  0.  0.  0.2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9000000000000001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acuracy per class - Class 1\n",
    "print(np.percentile(type_1_accuracy, [0, 25, 50, 75, 100]))\n",
    "\n",
    "np.mean(type_1_accuracy) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63414634 0.75609756 0.85365854 0.90243902 1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.51219512195121"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acuracy per class - Class 2\n",
    "print(np.percentile(type_2_accuracy, [0, 25, 50, 75, 100]))\n",
    "\n",
    "np.mean(type_2_accuracy) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on 100 iterations of a 2-hidden layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration =  0\n",
      "Iteration =  10\n",
      "Iteration =  20\n",
      "Iteration =  30\n",
      "Iteration =  40\n",
      "Iteration =  50\n",
      "Iteration =  60\n",
      "Iteration =  70\n",
      "Iteration =  80\n",
      "Iteration =  90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6173809523809524"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(100)\n",
    "\n",
    "per_epoch_loss = []\n",
    "test_accuracy_list = []\n",
    "type_0_accuracy = []\n",
    "type_1_accuracy = []\n",
    "type_2_accuracy = []\n",
    "\n",
    "for itr_count in range(100):\n",
    "\n",
    "    if itr_count%10 ==0:\n",
    "        print('Iteration = ', itr_count)\n",
    "    \n",
    "    # Generate test-train split\n",
    "    split_seed = rng.integers(2**30)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sub_matrix, response_variable, test_size=0.33\n",
    "                                                        , random_state=split_seed, stratify=response_variable)\n",
    "\n",
    "    # Use ONLY training set to estimate the parameters for data normalization\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[continuous_var])\n",
    "\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    X_train_scaled[continuous_var] = scaler.transform(X_train[continuous_var])\n",
    "    X_test_scaled[continuous_var] = scaler.transform(X_test[continuous_var])\n",
    "    grp_indexes = np.random.randint(num_batch, size=X_train_scaled.shape[0])\n",
    "\n",
    "    # Convert training and test data sets to tensors\n",
    "    X_train_scaled = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values)\n",
    "\n",
    "    X_test_scaled = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "\n",
    "    # Perform logistic regression\n",
    "    nnet_model = Hidden_Layer_Two(num_var)\n",
    "    nnet_loss_func = nn.CrossEntropyLoss(weight = weight_per_class) # Up- or down-sample observations to reduce class imbalance\n",
    "    nnet_optimizer = torch.optim.Adam(nnet_model.parameters(), lr=0.1)\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        for batch_idx in range(num_batch):\n",
    "            \n",
    "            current_batch_idx = np.where(grp_indexes == batch_idx)[0].tolist()\n",
    "            current_fit = nnet_model(X_train_scaled[current_batch_idx, :])  # 1\n",
    "            current_loss = nnet_loss_func(current_fit, y_train[current_batch_idx])  # 2\n",
    "            nnet_optimizer.zero_grad()  # 3\n",
    "            current_loss.backward()  # 4\n",
    "            nnet_optimizer.step()  # 5\n",
    "            per_epoch_loss.append(current_loss.detach().numpy().tolist())\n",
    "    \n",
    "    test_predictions = torch.argmax(torch.softmax(nnet_model(X_test_scaled), 1), axis=1).numpy()\n",
    "    test_accuracy = float(sum(test_predictions == y_test))/y_test.shape[0]\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "        \n",
    "np.mean(test_accuracy_list)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration =  0\n",
      "Iteration =  10\n",
      "Iteration =  20\n",
      "Iteration =  30\n",
      "Iteration =  40\n",
      "Iteration =  50\n",
      "Iteration =  60\n",
      "Iteration =  70\n",
      "Iteration =  80\n",
      "Iteration =  90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.566547619047619"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(100)\n",
    "\n",
    "test_accuracy_list = []\n",
    "type_0_accuracy = []\n",
    "type_1_accuracy = []\n",
    "type_2_accuracy = []\n",
    "\n",
    "# Set the parameters\n",
    "param = {'max_depth': 6, 'eta': 0.3, 'objective': 'multi:softmax', 'num_class': 3, 'eval_metric': 'mlogloss'}\n",
    "\n",
    "for itr_count in range(100):\n",
    "\n",
    "    if itr_count%10 ==0:\n",
    "        print('Iteration = ', itr_count)\n",
    "    \n",
    "    # Generate test-train split\n",
    "    split_seed = rng.integers(2**30)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sub_matrix, response_variable, test_size=0.33\n",
    "                                                        , random_state=split_seed, stratify=response_variable)\n",
    "\n",
    "    # Use ONLY training set to estimate the parameters for data normalization\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[continuous_var])\n",
    "\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    X_train_scaled[continuous_var] = scaler.transform(X_train[continuous_var])\n",
    "    X_test_scaled[continuous_var] = scaler.transform(X_test[continuous_var])\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test_scaled)\n",
    "    bst = xgb.train(param, dtrain, 10)\n",
    "    test_predictions = bst.predict(dtest)\n",
    "    \n",
    "    test_accuracy = float(sum(test_predictions == y_test))/y_test.shape[0]\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "        \n",
    "np.mean(test_accuracy_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
